Few-shot Examples:

Example 1: 

abstract: Text written by humans makes up the vast majority of the data used to pre-train and fine-tune large language models (LLMs). Many sources of this data—like code, forum posts, personal websites, and books—are easily attributed to one or a few “users”. In this paper, we ask if it is possible to infer if any of a _user’s_ data was used to train an LLM. Not only would this constitute a breach of privacy, but it would also enable users to detect when their data was used for training. We develop the first effective attacks for _user inference_—at times, with near-perfect success—against LLMs. Our attacks are easy to employ, requiring only black-box access to an LLM and a few samples from the user, which _need not be the ones that were trained on_. We find, both theoretically and empirically, that certain properties make users more susceptible to user inference: being an outlier, having highly correlated examples, and contributing a larger fraction of data. Based on these findings, we identify several methods for mitigating user inference including training with example-level differential privacy, removing within-user duplicate examples, and reducing a user’s contribution to the training data. Though these provide partial mitigation, our work highlights the need to develop methods to fully protect LLMs from user inference.

research question: How can we prevent language models from leaking whether a specific user's data was used during training?

approach: We can reduce the risk of user inference in language models by modifying both the training data and the training procedure to the unknown user specific signals. One main approach is to apply example level privacy preserving techniques such as differential privacy which will limit the model's ability to memorize patterns that are tied to individual users. For instance, adding carefully calibrated noise during training can help prevent the model from overfitting to any single user's writing style. Additionally, we can reduce susceptibility to inference attacks by minimizing data duplication from the same user and filtering out highly correlated samples and have to make sure that no single user dominates the training distribution. These steps make it harder for an attacker to distinguish whether a user's data was seen by the model even when they only have black-box access. Designing LLMs with these safeguards can help  to mitigate the privacy risks in large-scale pretraining settings.

proposed a new method: yes


Example 2:

abstract: In-context learning (ICL) adapts Large Language Models (LLMs) to new tasks, without requiring any parameter updates, but few annotated examples as input. In this work, we investigate selective annotation for ICL, where there is a limited budget for annotating examples, similar to low-budget active learning (AL). Although uncertainty-based selection is unreliable with few annotated data, we present CoverICL, an adaptive graph-based selection algorithm, that effectively incorporates uncertainty sampling into selective annotation for ICL. First, CoverICL builds a nearest-neighbor graph based on the semantic similarity between candidate ICL examples. Then, CoverICL employs uncertainty estimation by the LLM to identify hard examples for the task. Selective annotation is performed over the active graph of the hard examples, adapting the process to the particular LLM used and the task tackled. CoverICL selects the most representative examples by solving a Maximum Coverage problem, approximating diversity-based sampling. Extensive experiments on ten datasets and seven LLMs show that, by incorporating uncertainty via coverage on the active graph, CoverICL (1) outperforms existing AL methods for ICL by 2–4.6% accuracy points, (2) is up to 2x more budget-efficient than SOTA methods for low-budget AL, and (3) generalizes better across tasks compared to non-graph alternatives.
  
research question: How can we improve the efficiency of in-context learning under annotation budget constraints?
  
approach: We can improve the efficiency of in-context learning under limited annotation budgets by selecting the training examples that are both informative and diverse. One of the way to do this is to construct a similarity graph where each node represents a potential in-context example and edges represent semantic closeness. Then we have to identify a subset of examples that not only cover diverse parts of the input space but are also uncertain according to the language model’s predictions. For instance, we can score examples based on the prediction entropy and then we can select a small set that maximizes the coverage over this uncertainty-weighted graph. This allows the model to include the most challenging and representative samples in the context window by helping the LLMs to adapt to new tasks more effectively without needing any parameter updates.

proposed a new method: yes


Example 3:

abstract: Retrieval augmented generation (RAG) exhibits outstanding performance in promoting the knowledge capabilities of large language models (LLMs) with retrieved documents related to user queries. However, RAG only focuses on improving the response quality of LLMs via enhancing queries indiscriminately with retrieved information, paying little attention to what type of knowledge LLMs really need to answer original queries more accurately. In this paper, we suggest that long-tail knowledge is crucial for RAG as LLMs have already remembered common world knowledge during large-scale pre-training. Based on our observation, we propose a simple but effective long-tail knowledge detection method for LLMs. Specifically, the novel Generative Expected Calibration Error (GECE) metric is derived to measure the “long-tailness” of knowledge based on both statistics and semantics. Hence, we retrieve relevant documents and infuse them into the model for patching knowledge loopholes only when the input query relates to long-tail knowledge. Experiments show that, compared to existing RAG pipelines, our method achieves over 4x speedup in average inference time and consistent performance improvement in downstream tasks.

research question: How can we improve RAG beyond improving the response quality of LLMs via enhancing queries?
        
approach: We can improve retrieval-augmented generation by making it more selective in what knowledge it retrieves, focusing specifically on long-tail information that is under represented in the LLM’s pretraining. Since LLMs already encode a large amount of general knowledge, augmenting the queries with common information that may not significantly improves responses. Instead, we can design a metric that identifies when a query likely involves domain-specific content. For example, by determining how uncommon or semantically distinctive an idea is. When such long-tail knowledge is detected, the system can retrieve and insert only the most relevant documents into the generation pipeline. This approach helps to improve the quality of answers to queries that depend on less common knowledge. 
        
proposed a new method: yes


Example 4: 

abstract: Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection, called VADet. A variational autoencoding architecture based on language models is employed to learn from unlabelled data the topical information of the domain. Then, the model is fine-tuned with a few manually annotated examples of user attitudes. We validate the effectiveness of VADet on our annotated data and also on an existing vaccination corpus annotated with opinions on vaccines. Our results show that VADet is able to learn disentangled stance and aspect topics, and outperforms existing aspect-based sentiment analysis models on both stance detection and tweet clustering.
        
research question: How can we detect vaccine attitudes on social media using limited annotated data?
        
approach: We can detect vaccine-related attitudes from social media by combining the strengths of semi-supervised learning with latent topic modeling. We can start by using unannotated social media posts to learn about the domain-specific topics and information through a variational auto encoding architecture. This step helps us to capture the patterns in user language without depending on the pre-labeled categories. Then, we can fine-tune the model using a small number of annotated examples to calibrate the latent representations for supervised tasks such as text classification. This approach can reduces the annotation costs and enables the generalization to emerging topics and mixed sentiments in complex domains like public health discourse.

proposed a new method: yes

Example 5:

abstract: We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.

proposed a new method: no


Example 6:        
        
abstract: We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.
        
research question: How can we enhance the fine-tuning of language models to be more resource-efficient while effectively using pre-trained knowledge?
        
approach: We can enhance the fine-tuning of language models by focusing on modifying only a small and strategically chosen subset of the model's parameters instead of just updating the entire model. One of the best strategy is to adjust the bias terms that reduces the number of trainable parameters. This approach retains most of the original model's structure and depends on the idea that pre-trained language models already encode a broad spectrum of general knowledge. Instead of teaching the model a new knowledge from scratch, fine-tuning in this way helps surface relevant information already learned during pretraining. This method allows for fast adaptation to new tasks with lower memory and compute requirements and making it ideal for situations with limited resources.

proposed a new method: yes


Example 7:

abstract: Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing, and syntactic generalization tests while using fewer parameters than other models.
        
research question: How can we design language models that better incorporate syntactic structure to improve generalization and reduce overparameterization?
        
approach: We can enhance language models by explicitly using syntactic structure into the learning and generation process. One of the best method is to integrate an incremental parser within the model by allowing it to construct a syntactic representation of the input as it processes the sentence left to right. This mechanism helps the model to make more informed predictions by improving its ability to generalize across linguistic patterns. To make sure that the parser remains robust during training especially when early parsing decisions may be incorrect, we can use a dynamic supervision strategy that provides corrective feedback such as adjusting future parsing choices based on the evolving context. This kind of syntax-aware modeling can lead to more compact and interpretable representations.

proposed a new method: yes

Example 8:

abstract: We investigate the effect of sub-word tokenization on representations of German noun compounds: single orthographic words which are composed of two or more constituents but often tokenized into units that are not morphologically motivated or meaningful. Using variants of BERT models and tokenization strategies on domain-specific restricted diachronic data, we introduce a suite of evaluations relying on the masked language modelling task and compositionality prediction. We obtain the most consistent improvements by pre-splitting compounds into constituents.

proposed a new method: no
